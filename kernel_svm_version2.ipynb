{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "               input_size = 1,\n",
    "               embedding_size = 1,\n",
    "               hidden_size = 16,\n",
    "               n_layers = 2,\n",
    "               dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.linear = nn.Linear(input_size, embedding_size, n_layers)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, \n",
    "                        dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input batch data, \n",
    "        size of x: [sequence len, batch size, feature size]\n",
    "        \"\"\"\n",
    "\n",
    "        # size of embedded : [sequence len, batch size, embedding size]\n",
    "        embedded = self.dropout(F.relu(self.linear(x)))\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # hidden: the last step hidden of each layer of rnn\n",
    "        # size of hidden : [num of layers * num directions, batch size, hidden size]\n",
    "        # num of directions is 1, since we are useing signle directional rnn\n",
    "        # cell: the last step cell of each layer of rnn\n",
    "        # size of cell: [num of layers * num of directions, batch size, hidden size]\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                output_size = 1,\n",
    "                embedding_size = 1,\n",
    "                hidden_size = 16,\n",
    "                n_layers = 2,\n",
    "                dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Linear(output_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, dropout = dropout)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        \"\"\"\n",
    "        x: input batch data, \n",
    "        size of x: [batch size, feature size]\n",
    "        x is only 2-dimensional, since the input is batches of last coordinate of the sequence,\n",
    "        so the sequence length has been removed\n",
    "        \"\"\"\n",
    "\n",
    "        # add a sequence dimension to the front of x, to allow for use of nn.LSTM method\n",
    "        x = x.unsqueeze(0)\n",
    "        # size(x) now becomes [1, batch size, feature size]\n",
    "        embedded = self.dropout(F.relu(self.embedding(x)))\n",
    "\n",
    "        # size of output : [seq len, batch size, hidden dimension * num of directions]\n",
    "        # size of hidden : [num of layers * num of directions, batch size, hidden dim]\n",
    "        # size of cell : [num of layers * num of directions, batch size, hidden dim]\n",
    "\n",
    "        # notice that sequence len and num of directions will always be 1 in the Decoder, therfore:\n",
    "        # size of output : [1, batch size, hidden dimension]\n",
    "        # size of hidden : [num of layers, batch size, hidden dim]\n",
    "        # size of cell : [num of directions, batch size, hidden dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        # prediction = [batch size, output size]\n",
    "        prediction = self.linear(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Seq2seq\n",
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        #self.device = device\n",
    "\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        size of x : [observed sequence len, batch size, feature size]\n",
    "        size of y : [target sequence len, batch size, feature size]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[1]\n",
    "        target_len = y.shape[0]\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step\n",
    "        outputs = torch.zeros(y.shape)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(x)\n",
    "\n",
    "        # first input to decoder is last coordinates of x\n",
    "        decoder_input = x[-1, :, :]\n",
    "        \n",
    "        for i in range(target_len):\n",
    "            # run decode for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            \n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[i] = output\n",
    "\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # output is the same shape as input, [batch_size, feature size]\n",
    "            # use output directly as input or use true lable depending on\n",
    "            # teacher_forcing is true or not\n",
    "            decoder_input = y[i] if teacher_forcing else output\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: RBF\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RBF, self).__init__()\n",
    "        self.sigma = nn.Parameter(torch.Tensor(1))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.constant_(self.sigma, 1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        '''\n",
    "        size of x1/x2 : [input sequence len, batch size, feature size],\n",
    "        for our task, the last two sizes are both 1.\n",
    "        '''\n",
    "        \n",
    "        value = (x1 - x2).pow(2).sum(0).pow(0.5) / self.sigma\n",
    "        \n",
    "        return torch.exp(-value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Kernel\n",
    "# class Kernel refers to the structure combining Seq2seq module and RBF module.\n",
    "class Kernel(nn.Module):\n",
    "    def __init__(self, seq2seq, rbf, target_length, output_dim):\n",
    "        super().__init__()\n",
    "        # target_length: seq2seq2 output sequence length\n",
    "        self.target_length = target_length\n",
    "        # output_dim: seq2seq2 output embedding size; in our case being 1\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.seq2seq = seq2seq\n",
    "        self.rbf = rbf\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        size of x1/x2 : [observed sequence len, batch size, feature size]\n",
    "        \"\"\"\n",
    "        # size of output_size corresponds to the size of seq2seq output\n",
    "        output_size = torch.randn(self.target_length, 1, self.output_dim)\n",
    "        \n",
    "        outputs1 = self.seq2seq(x1, output_size)\n",
    "        outputs2 = self.seq2seq(x2, output_size)\n",
    "        \n",
    "        # size of value : [batch size, feature size], both being 1 in our case\n",
    "        value = self.rbf(outputs1, outputs2)\n",
    "        \n",
    "        return value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, kernel, xs, ys):\n",
    "        super().__init__()\n",
    "        # xs is a list of input data x,\n",
    "        # where the size of x is: [sequence len, batch size, feature size]\n",
    "        self.xs = xs\n",
    "        # ys is a list of label y.\n",
    "        # size of y: [batch size]\n",
    "        self.ys = ys\n",
    "        # data_length: num of items\n",
    "        self.data_length = len(ys)\n",
    "        # size of alphas: [num of items, batch size]\n",
    "        self.alphas = torch.randn(self.data_length, 1)\n",
    "        self.kernel = kernel\n",
    "        #self.kernel_np = lambda x1, x2: kernel(x1, x2).detach().numpy()\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        value = torch.zeros(1,1)\n",
    "        \n",
    "        for i in range(self.data_length):\n",
    "            for j in range(self.data_length):\n",
    "                # the i-j term of dual kernal-svm objective\n",
    "                term = self.alphas[i]*self.alphas[j]*self.ys[i]*self.ys[j]*self.kernel(self.xs[i],self.xs[j])  \n",
    "                value = torch.add(value, term)\n",
    "        \n",
    "        value = -0.5*value\n",
    "        print(f\"objective value now is {value}\\n\")\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def update_alpha(self, alphas):\n",
    "        self.alphas = alphas\n",
    "        \n",
    "    def kernel_np(self, x1, x2):\n",
    "        '''\n",
    "        kernel_np takes the argument of 2-dimensional list,\n",
    "        tranforming them into tensors demanded by kernel.forward(),\n",
    "        returning a 1-d numpy array containing one single number\n",
    "        \n",
    "        x1 is a list of lists: [ [2, 5, 18, 0, 2, ...(sequence of data item 1)], [(sequence of data item 2)], .... ]\n",
    "        '''\n",
    "        lst1 = []\n",
    "        lst2 = []\n",
    "        \n",
    "        for item in x1:\n",
    "            lst1.append([[item]])\n",
    "        x1_alter = torch.Tensor(lst1)\n",
    "        \n",
    "        lst2 = []\n",
    "        for item in x2:\n",
    "            lst2.append([[item]])\n",
    "        x1_alter = torch.Tensor(lst1)\n",
    "        x2_alter = torch.Tensor(lst2)\n",
    "        \n",
    "        #print(x1_alter)\n",
    "        #print(x2_alter)\n",
    "        return kernel(x1_alter, x2_alter).detach().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devivce= cpu\n"
     ]
    }
   ],
   "source": [
    "# block 7, initialize the model\n",
    "\n",
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 1\n",
    "DEC_EMB_DIM = 1\n",
    "HID_DIM = 16   # this can be adjusted\n",
    "N_LAYERS = 2   # this can be adjusted\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "TARG_LENGTH = 16 # this can be adjusted\n",
    "\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"devivce=\", dev)\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "seq = Seq2seq(enc, dec)\n",
    "rbf = RBF()\n",
    "kernel = Kernel(seq, rbf, TARG_LENGTH, OUTPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 11, 26, 11, 25, 8, 16, 12, 19, 12, 5, 7, 21, 13, 10, 13, 24, 22, 19, 22, 0, 8, 20, 21, 12, 15, 17], [21, 20, 15, 21, 21, 0, 4, 21, 22, 21, 19, 7, 9, 1, 25, 10, 22, 7, 19, 15, 23, 15, 3, 16], [25, 24, 4, 12, 2, 1, 0, 5, 7, 20, 23, 20, 5, 16, 6, 13, 6, 4, 20, 7, 16, 11, 10, 24, 2, 24, 6, 26, 0, 11, 2, 10, 15, 22, 19, 10], [19, 17, 17, 20, 1, 17, 21, 18, 19, 7, 17, 5, 17, 3, 24, 16, 13, 11, 11, 26, 22, 25, 22, 9, 14, 3, 18, 10, 19, 22, 0, 1, 4, 14, 21, 15], [0, 7, 5, 2, 11, 10, 14, 18, 4, 20, 26, 7, 26, 11, 7, 22, 13, 0, 8, 10, 7, 9, 3, 20, 25, 7, 21, 20, 25, 5, 16, 26, 6, 20, 11, 14, 24, 6, 3, 20, 8, 10, 25], [14, 22, 11, 0, 12, 6, 20, 8, 16, 13, 26, 11, 20, 25, 7, 18, 10, 20, 25, 3, 1, 17, 18, 25, 23, 23, 21, 21, 24, 1, 21, 2, 21, 20, 20, 6, 2, 6, 17, 12, 21, 16, 9, 2], [7, 0, 4, 5, 4, 3, 21, 9, 20, 15, 21, 18, 13, 15, 25, 12, 6, 12, 11, 3, 3, 1], [23, 25, 14, 10, 4, 0, 3, 6, 15, 5, 7, 0, 20, 7, 3, 9, 5, 23, 1, 12, 16, 18, 8, 24, 2, 3, 9, 4, 6, 2, 13, 25, 20], [18, 10, 0, 20, 1, 20, 5, 10, 20, 2, 5, 12, 4, 26, 13, 19, 3, 2, 8, 8, 4, 14, 11, 7, 15, 18, 26, 23, 24, 13, 21, 6, 11, 22, 16, 6, 14], [19, 2, 5, 22, 9, 11, 4, 24, 6, 1, 22, 23, 14, 10, 19, 24, 12, 15, 1, 6, 16, 25, 7, 11, 0, 9, 10, 14, 11, 12]]\n",
      "[0, 1, 0, 1, 1, 0, 1, 1, 1, 0]\n",
      "[tensor([[[ 3.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[17.]]]), tensor([[[21.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[16.]]]), tensor([[[25.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[10.]]]), tensor([[[19.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[15.]]]), tensor([[[ 0.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[25.]]]), tensor([[[14.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 2.]]]), tensor([[[ 7.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 1.]]]), tensor([[[23.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[20.]]]), tensor([[[18.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[ 8.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[26.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[21.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[14.]]]), tensor([[[19.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[22.]],\n",
      "\n",
      "        [[23.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[19.]],\n",
      "\n",
      "        [[24.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[15.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 0.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[10.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[12.]]])]\n",
      "[tensor([0.]), tensor([1.]), tensor([0.]), tensor([1.]), tensor([1.]), tensor([0.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([0.])]\n",
      "Model(\n",
      "  (kernel): Kernel(\n",
      "    (seq2seq): Seq2seq(\n",
      "      (encoder): Encoder(\n",
      "        (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (rnn): LSTM(1, 16, num_layers=2, dropout=0.5)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (embedding): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (rnn): LSTM(1, 16, num_layers=2, dropout=0.5)\n",
      "        (linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (rbf): RBF()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96276796], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block 8, generating random test sample\n",
    "\n",
    "datanum = 10\n",
    "X_trainSVM = []\n",
    "Y_trainSVM = []\n",
    "for i in range(0,datanum):\n",
    "    lst = []\n",
    "    length = random.randint(20,50)\n",
    "    for j in range(0,length):\n",
    "        lst.append(random.randint(0,26))\n",
    "    X_trainSVM.append(lst)\n",
    "\n",
    "for i in range(0,datanum):\n",
    "    Y_trainSVM.append(random.randint(0,1))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for item in X_trainSVM:\n",
    "    lst = []\n",
    "    for num in item:\n",
    "        lst.append([[num]])\n",
    "    X_train.append(torch.Tensor(lst))\n",
    "\n",
    "for num in Y_trainSVM:\n",
    "    Y_train.append(torch.Tensor([num]))\n",
    "\n",
    "    \n",
    "print(X_trainSVM)\n",
    "print(Y_trainSVM)\n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "    \n",
    "model = Model(kernel, X_train, Y_train).to(dev)\n",
    "print(model)\n",
    "\n",
    "kernel(X_train[0],X_train[1])\n",
    "model.kernel_np(X_trainSVM[0], X_trainSVM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[17.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[18.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[25.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[13.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[20.]],\n",
      "\n",
      "        [[12.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 5.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[16.]],\n",
      "\n",
      "        [[14.]],\n",
      "\n",
      "        [[ 7.]],\n",
      "\n",
      "        [[ 6.]]])\n",
      "tensor([1.])\n",
      "[4, 25, 11, 4, 4, 4, 4, 11, 12, 11, 16, 4, 6, 3, 6, 12, 5, 5, 4, 16, 7, 9, 3, 18, 7, 25, 9, 20, 18, 25, 6, 25, 14, 14, 17, 20, 11, 17, 3, 5, 18, 6, 11, 25, 7, 7, 3, 12, 7, 14, 13, 14, 14, 6, 5, 20, 12, 5, 5, 3, 11, 14, 9, 3, 5, 4, 7, 16, 14, 7, 6]\n",
      "1\n",
      "Model(\n",
      "  (kernel): Kernel(\n",
      "    (seq2seq): Seq2seq(\n",
      "      (encoder): Encoder(\n",
      "        (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (rnn): LSTM(1, 16, num_layers=2, dropout=0.5)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (embedding): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (rnn): LSTM(1, 16, num_layers=2, dropout=0.5)\n",
      "        (linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (rbf): RBF()\n",
      "  )\n",
      ")\n",
      "tensor([[ 7.7424e-01],\n",
      "        [ 1.1186e+00],\n",
      "        [-9.4683e-02],\n",
      "        [-2.4796e-01],\n",
      "        [ 4.1197e-01],\n",
      "        [-1.9965e-01],\n",
      "        [ 1.5488e+00],\n",
      "        [-2.9129e-01],\n",
      "        [-5.2774e-03],\n",
      "        [ 1.2662e+00],\n",
      "        [-2.7349e+00],\n",
      "        [ 1.1516e-01],\n",
      "        [-5.9940e-01],\n",
      "        [-4.8273e-01],\n",
      "        [-2.0193e-01],\n",
      "        [ 1.6359e-01],\n",
      "        [-7.5065e-01],\n",
      "        [ 4.0218e-01],\n",
      "        [-2.2096e+00],\n",
      "        [ 2.5559e-01],\n",
      "        [ 1.0197e+00],\n",
      "        [ 8.9370e-01],\n",
      "        [ 3.1394e-01],\n",
      "        [ 8.1390e-01],\n",
      "        [ 6.2934e-02],\n",
      "        [-1.1955e-02],\n",
      "        [-2.3652e+00],\n",
      "        [ 3.4693e-01],\n",
      "        [-2.1161e-01],\n",
      "        [-1.6104e+00],\n",
      "        [ 1.9149e+00],\n",
      "        [-7.2877e-01],\n",
      "        [ 2.0261e-01],\n",
      "        [-1.3739e+00],\n",
      "        [ 1.3326e+00],\n",
      "        [ 7.1488e-01],\n",
      "        [ 2.6162e-02],\n",
      "        [-2.6263e-01],\n",
      "        [ 2.5178e-01],\n",
      "        [-4.7550e-01],\n",
      "        [-1.0317e+00],\n",
      "        [-1.6936e-01],\n",
      "        [ 4.7463e-01],\n",
      "        [ 7.9060e-01],\n",
      "        [-7.1806e-01],\n",
      "        [-2.4804e-01],\n",
      "        [-9.0444e-01],\n",
      "        [-1.2877e+00],\n",
      "        [-3.0818e-01],\n",
      "        [-1.1391e+00],\n",
      "        [ 1.4584e+00],\n",
      "        [ 7.4456e-01],\n",
      "        [ 1.2418e-01],\n",
      "        [ 1.5890e+00],\n",
      "        [ 1.1734e+00],\n",
      "        [-2.2538e-01],\n",
      "        [ 8.7462e-01],\n",
      "        [ 1.1992e+00],\n",
      "        [-1.7857e+00],\n",
      "        [-7.6461e-01],\n",
      "        [ 2.9127e-02],\n",
      "        [-1.8701e+00],\n",
      "        [-4.0918e-01],\n",
      "        [-5.0740e-02],\n",
      "        [-8.3836e-03],\n",
      "        [ 3.4023e-01],\n",
      "        [-4.9829e-01],\n",
      "        [ 2.9752e-01],\n",
      "        [ 1.3665e+00],\n",
      "        [ 4.2450e-01],\n",
      "        [-1.6263e-01],\n",
      "        [-1.5762e+00],\n",
      "        [ 5.1719e-01],\n",
      "        [-8.2396e-01],\n",
      "        [ 1.8518e+00],\n",
      "        [ 7.4895e-01],\n",
      "        [ 1.2824e+00],\n",
      "        [ 3.4871e-02],\n",
      "        [-6.1475e-02],\n",
      "        [ 1.5798e+00],\n",
      "        [ 7.6565e-01],\n",
      "        [-8.9418e-01],\n",
      "        [ 5.7927e-01],\n",
      "        [ 9.8810e-01],\n",
      "        [ 2.1341e-01],\n",
      "        [-2.3923e-01],\n",
      "        [ 7.0449e-01],\n",
      "        [-7.3387e-01],\n",
      "        [-5.2166e-01],\n",
      "        [ 6.0482e-01],\n",
      "        [ 6.8440e-01],\n",
      "        [-2.0033e+00],\n",
      "        [-6.2050e-01],\n",
      "        [ 1.0570e+00],\n",
      "        [ 2.5549e-01],\n",
      "        [ 2.0933e-01],\n",
      "        [-2.6701e+00],\n",
      "        [-5.8337e-02],\n",
      "        [ 9.4205e-01],\n",
      "        [-1.8254e+00],\n",
      "        [ 7.5263e-01],\n",
      "        [ 3.9202e-01],\n",
      "        [-5.9400e-01],\n",
      "        [ 1.1238e+00],\n",
      "        [-1.1146e+00],\n",
      "        [-1.6343e-01],\n",
      "        [ 5.4767e-01],\n",
      "        [ 8.1192e-01],\n",
      "        [-5.2730e-02],\n",
      "        [-7.0898e-01],\n",
      "        [-7.0414e-01],\n",
      "        [ 9.4901e-01],\n",
      "        [ 4.9998e-01],\n",
      "        [-1.5626e+00],\n",
      "        [ 1.0404e+00],\n",
      "        [-1.2453e+00],\n",
      "        [ 1.6412e-01],\n",
      "        [-8.2267e-01],\n",
      "        [ 3.6419e-01],\n",
      "        [ 1.1789e+00],\n",
      "        [ 1.4759e+00],\n",
      "        [ 3.4683e-01],\n",
      "        [-4.4927e-01],\n",
      "        [-3.7885e-01],\n",
      "        [-1.4782e+00],\n",
      "        [ 7.3940e-01],\n",
      "        [-1.0203e+00],\n",
      "        [ 6.1890e-01],\n",
      "        [-7.2635e-01],\n",
      "        [-9.2908e-01],\n",
      "        [ 2.3926e-01],\n",
      "        [ 1.1498e+00],\n",
      "        [ 4.1025e-01],\n",
      "        [-6.9158e-01],\n",
      "        [ 6.6688e-01],\n",
      "        [-1.5922e+00],\n",
      "        [ 6.7729e-01],\n",
      "        [ 9.0450e-01],\n",
      "        [-6.4323e-01],\n",
      "        [ 4.1493e-01],\n",
      "        [ 4.6154e-01],\n",
      "        [-4.0101e-01],\n",
      "        [ 2.2434e+00],\n",
      "        [ 8.0790e-02],\n",
      "        [-5.8383e-01],\n",
      "        [ 1.2970e-02],\n",
      "        [-3.4986e-01],\n",
      "        [ 4.6311e-01],\n",
      "        [ 8.4108e-01],\n",
      "        [-1.0818e+00],\n",
      "        [ 1.2343e+00],\n",
      "        [ 2.0701e+00],\n",
      "        [ 1.0637e+00],\n",
      "        [ 2.1062e-01],\n",
      "        [-2.7326e-01],\n",
      "        [-1.6701e+00],\n",
      "        [-6.8170e-01],\n",
      "        [ 1.3861e+00],\n",
      "        [-7.6101e-01],\n",
      "        [-1.1815e+00],\n",
      "        [-1.2765e+00],\n",
      "        [-6.9945e-01],\n",
      "        [-8.4656e-01],\n",
      "        [ 1.4023e+00],\n",
      "        [ 8.8984e-01],\n",
      "        [ 7.3895e-01],\n",
      "        [ 7.9537e-01],\n",
      "        [-3.9006e-01],\n",
      "        [-3.0439e-01],\n",
      "        [ 2.3599e-01],\n",
      "        [ 5.0160e-01],\n",
      "        [-1.0588e-02],\n",
      "        [ 4.9720e-01],\n",
      "        [-2.8979e-02],\n",
      "        [-2.1794e-01],\n",
      "        [-1.2824e-01],\n",
      "        [ 1.2778e+00],\n",
      "        [-1.7210e+00],\n",
      "        [ 1.0130e+00],\n",
      "        [-2.4797e-01],\n",
      "        [ 4.3127e-02],\n",
      "        [-8.3226e-02],\n",
      "        [ 4.3267e-01],\n",
      "        [-9.8967e-01],\n",
      "        [ 1.4097e+00],\n",
      "        [-7.7172e-01],\n",
      "        [-1.9506e+00],\n",
      "        [ 4.7065e-01],\n",
      "        [-1.6080e-01],\n",
      "        [ 6.0466e-01],\n",
      "        [ 4.3157e-01],\n",
      "        [-1.1971e+00],\n",
      "        [ 1.7337e+00],\n",
      "        [-8.3888e-02],\n",
      "        [-2.8347e-01],\n",
      "        [ 1.3575e+00],\n",
      "        [-3.7430e-01],\n",
      "        [-1.6152e+00],\n",
      "        [-1.3801e+00],\n",
      "        [-4.0050e-01],\n",
      "        [ 3.9328e-01],\n",
      "        [ 6.1579e-01],\n",
      "        [-5.2273e-01],\n",
      "        [ 4.8331e-01],\n",
      "        [-2.7503e+00],\n",
      "        [-3.9530e-01],\n",
      "        [-5.5561e-01],\n",
      "        [ 3.6096e-01],\n",
      "        [-3.3212e-02],\n",
      "        [ 1.5347e-01],\n",
      "        [-1.5362e-01],\n",
      "        [ 1.5945e+00],\n",
      "        [-2.7156e-01],\n",
      "        [-2.3609e-01],\n",
      "        [-9.4209e-01],\n",
      "        [ 6.6057e-01],\n",
      "        [ 2.3596e-01],\n",
      "        [ 1.9525e+00],\n",
      "        [-2.1205e-01],\n",
      "        [ 1.1446e+00],\n",
      "        [ 9.8202e-01],\n",
      "        [ 5.0080e-01],\n",
      "        [ 5.8051e-01],\n",
      "        [-1.0504e-01],\n",
      "        [-5.1225e-01],\n",
      "        [-7.7247e-01],\n",
      "        [-9.9153e-01],\n",
      "        [ 4.6948e-01],\n",
      "        [-3.9583e-01],\n",
      "        [-1.5789e-01],\n",
      "        [ 1.0330e+00],\n",
      "        [-7.4997e-01],\n",
      "        [ 1.4972e+00],\n",
      "        [ 9.1873e-01],\n",
      "        [ 1.2149e+00],\n",
      "        [ 1.3303e+00],\n",
      "        [-5.0166e-01],\n",
      "        [-6.7799e-01],\n",
      "        [ 1.4965e+00],\n",
      "        [ 1.3773e+00],\n",
      "        [-1.0992e-01],\n",
      "        [-7.0369e-01],\n",
      "        [ 9.1967e-01],\n",
      "        [-5.6443e-01],\n",
      "        [-5.3242e-01],\n",
      "        [-9.3090e-01],\n",
      "        [-7.7452e-01],\n",
      "        [ 1.6539e+00],\n",
      "        [-5.3727e-01],\n",
      "        [ 3.6432e-01],\n",
      "        [ 4.1283e-01],\n",
      "        [-3.2855e-01],\n",
      "        [ 1.9337e+00],\n",
      "        [ 6.1211e-01],\n",
      "        [ 5.1964e-01],\n",
      "        [ 8.1035e-01],\n",
      "        [ 3.3118e-01],\n",
      "        [-3.7396e-01],\n",
      "        [ 6.2948e-01],\n",
      "        [-2.1379e-02],\n",
      "        [-1.0763e+00],\n",
      "        [ 1.5791e+00],\n",
      "        [-9.3894e-01],\n",
      "        [-1.1114e+00],\n",
      "        [-8.0670e-01],\n",
      "        [-1.6801e+00],\n",
      "        [-9.2276e-01],\n",
      "        [ 6.4599e-01],\n",
      "        [ 7.2657e-01],\n",
      "        [ 5.1984e-01],\n",
      "        [-7.7924e-01],\n",
      "        [-1.9952e-02],\n",
      "        [-6.4088e-01],\n",
      "        [-1.8507e-01],\n",
      "        [-4.8783e-01],\n",
      "        [-1.7892e-01],\n",
      "        [-1.0046e+00],\n",
      "        [-8.4276e-01],\n",
      "        [-9.4697e-01],\n",
      "        [-1.1113e+00],\n",
      "        [-1.0139e-02],\n",
      "        [ 2.8440e-01],\n",
      "        [ 2.7434e-01],\n",
      "        [-2.7973e-01],\n",
      "        [-8.6266e-01],\n",
      "        [ 2.6076e-03],\n",
      "        [-9.5381e-02],\n",
      "        [ 6.9316e-01],\n",
      "        [-1.3879e+00],\n",
      "        [-5.1560e-01],\n",
      "        [ 1.8177e-01],\n",
      "        [-1.4260e+00],\n",
      "        [ 1.5032e+00],\n",
      "        [ 3.2130e-01],\n",
      "        [-4.1306e-01],\n",
      "        [-6.4352e-01],\n",
      "        [ 9.3829e-01],\n",
      "        [ 6.5910e-01],\n",
      "        [ 1.1989e+00],\n",
      "        [-1.3959e+00],\n",
      "        [-3.0875e-01],\n",
      "        [ 5.5618e-01],\n",
      "        [ 2.0979e+00],\n",
      "        [ 1.6903e+00],\n",
      "        [ 1.2017e+00],\n",
      "        [ 1.5701e-01],\n",
      "        [ 4.7504e-01],\n",
      "        [-1.0329e+00],\n",
      "        [-3.1815e-01],\n",
      "        [-1.3581e+00],\n",
      "        [-1.4050e-01],\n",
      "        [-6.3954e-01],\n",
      "        [ 6.9214e-01],\n",
      "        [-1.7925e+00],\n",
      "        [ 4.6693e-02],\n",
      "        [-6.4362e-01],\n",
      "        [ 2.3860e-02],\n",
      "        [-2.4362e-01],\n",
      "        [-2.0649e-01],\n",
      "        [ 3.7090e-01],\n",
      "        [-1.6536e-01],\n",
      "        [ 6.6383e-01],\n",
      "        [ 9.2807e-01],\n",
      "        [-4.8603e-02],\n",
      "        [ 5.5560e-01],\n",
      "        [ 3.3205e-01],\n",
      "        [ 6.4621e-01],\n",
      "        [ 1.7166e+00],\n",
      "        [ 1.3661e+00],\n",
      "        [-3.9935e-01],\n",
      "        [-1.1365e+00],\n",
      "        [-8.4627e-01],\n",
      "        [ 1.1561e+00],\n",
      "        [-1.5618e+00],\n",
      "        [-7.9300e-01],\n",
      "        [-2.0241e-01],\n",
      "        [ 5.9322e-01],\n",
      "        [ 1.6120e+00],\n",
      "        [ 1.0481e+00],\n",
      "        [ 1.0415e-01],\n",
      "        [-1.7701e+00],\n",
      "        [-7.7224e-01],\n",
      "        [-4.9931e-01],\n",
      "        [-6.1742e-01],\n",
      "        [-6.6345e-01],\n",
      "        [ 3.4503e-01],\n",
      "        [ 1.3041e+00],\n",
      "        [ 3.5453e-01],\n",
      "        [ 6.4306e-01],\n",
      "        [-1.0359e-01],\n",
      "        [ 2.8797e-02],\n",
      "        [ 5.9389e-01],\n",
      "        [-2.0362e-01],\n",
      "        [-6.8242e-02],\n",
      "        [ 8.8175e-01],\n",
      "        [ 4.3221e-01],\n",
      "        [ 6.8183e-02],\n",
      "        [-7.6676e-01],\n",
      "        [-6.6272e-02],\n",
      "        [ 1.0634e-01],\n",
      "        [ 1.3014e-01],\n",
      "        [ 6.6949e-01],\n",
      "        [-1.4380e-01],\n",
      "        [-1.1016e+00],\n",
      "        [ 8.0481e-01],\n",
      "        [-1.2905e-01],\n",
      "        [-2.0725e+00],\n",
      "        [-1.2445e-02],\n",
      "        [ 4.0305e-01],\n",
      "        [ 1.0708e+00],\n",
      "        [-1.4209e-02],\n",
      "        [-9.9676e-01],\n",
      "        [ 5.7982e-01],\n",
      "        [-1.1110e+00]])\n"
     ]
    }
   ],
   "source": [
    "# block 9\n",
    "# this is the interface to incorporate traing datasets into the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train, Y_train stores the data demanded by the -------NETWORK---------\n",
    "# formats:\n",
    "# X_train should be a list of input data x,\n",
    "# where x should be a torch.tensor, size: [sequence len, batch size, feature size]\n",
    "# Y_train should be a list of label y.\n",
    "# where y should be a torch.tensor, size: [batch size]\n",
    "# in our case: batch size and feature size are both set to 1\n",
    "# refer to block 6 to view the details\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# X_trainSVM, Y_trainSVM stores the data demanded by the -------SVM OPTIMIZER---------\n",
    "# formats:\n",
    "# X_trainSVM should be a list of input data x,\n",
    "# where x is a list of number\n",
    "# Y_trainSVM should be a list of label (number)\n",
    "X_trainSVM = []\n",
    "Y_trainSVM = []\n",
    "\n",
    "import json\n",
    "# replace the file to be loaded here.\n",
    "with open('shuffle.json', 'r') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "# load X_train, Y_train\n",
    "for key in data:\n",
    "    lst = []\n",
    "    for item in data[key]['x']:\n",
    "        lst.append([[item]])\n",
    "    X_train.append(torch.Tensor(lst))\n",
    "    Y_train.append(torch.Tensor([data[key]['y']]))\n",
    "print(X_train[0])\n",
    "print(Y_train[0])\n",
    "\n",
    "# load X_trainSVM, Y_trainSVM\n",
    "for key in data:\n",
    "    X_trainSVM.append(data[key]['x'])\n",
    "    Y_trainSVM.append(data[key]['y']) \n",
    "print(X_trainSVM[0])\n",
    "print(Y_trainSVM[0])\n",
    "\n",
    "model = Model(kernel, X_train, Y_train).to(dev)\n",
    "print(model)\n",
    "print(model.alphas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm.svm import SVM\n",
    "svm = SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training begin\n",
      "rounds = 1 \n",
      "\n",
      "objective value now is tensor([[-0.7287]], grad_fn=<MulBackward0>)\n",
      "\n",
      "loss = tensor([[-0.7287]], grad_fn=<MulBackward0>)\n",
      "\n",
      "svm score is: 0.4\n",
      "new alphas are: tensor([[1000.],\n",
      "        [   0.],\n",
      "        [1000.],\n",
      "        [   0.],\n",
      "        [   0.],\n",
      "        [1000.],\n",
      "        [   0.],\n",
      "        [   0.],\n",
      "        [   0.],\n",
      "        [1000.]], dtype=torch.float64)\n",
      "rounds = 2 \n",
      "\n",
      "objective value now is tensor([[-0.]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "\n",
      "loss = tensor([[-0.]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-b33fcc7f7b74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m                              \u001b[0mY_trainSVM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                              kernel=model.kernel_np)\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"svm score is: {svm.score(X_trainSVM, Y_trainSVM)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m# -------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0malpha_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\sl2021-main\\svm\\svm.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, test_data, test_label)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mcorrect\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\sl2021-main\\svm\\svm.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, instance)\u001b[0m\n\u001b[0;32m     65\u001b[0m         '''\n\u001b[0;32m     66\u001b[0m         return np.sign(self.intercept_ + np.sum(\n\u001b[1;32m---> 67\u001b[1;33m             [self.alpha[i] * self.train_label[i] * self.kernel(instance, self.train_data[i]) for i in\n\u001b[0m\u001b[0;32m     68\u001b[0m              range(len(self.train_label))]))\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\sl2021-main\\svm\\svm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m         '''\n\u001b[0;32m     66\u001b[0m         return np.sign(self.intercept_ + np.sum(\n\u001b[1;32m---> 67\u001b[1;33m             [self.alpha[i] * self.train_label[i] * self.kernel(instance, self.train_data[i]) for i in\n\u001b[0m\u001b[0;32m     68\u001b[0m              range(len(self.train_label))]))\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-19a92d7afd7e>\u001b[0m in \u001b[0;36mkernel_np\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mlst1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mx1_alter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlst1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mlst2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# block 10, training.\n",
    "\n",
    "class myCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, output):\n",
    "        return output\n",
    "    \n",
    "criterion = myCustom()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"training begin\")\n",
    "rounds = 0\n",
    "for rounds in range(1000):\n",
    "    for epoch in range(5):\n",
    "        # zero the parameter gradients\n",
    "        rounds += 1\n",
    "        print(f\"rounds = {rounds} \\n\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize kernel-svm objective wrt \\theta (kernel parameters)\n",
    "        output = model()\n",
    "        loss = criterion(output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"loss = {loss}\\n\")\n",
    "        \n",
    "        \n",
    "        # call kernel-svm solver to optimize objective wrt alpha (at the same time, passing the current kernel)\n",
    "        # returns the updated new_alphas\n",
    "        new_alphas = svm.fit(X_trainSVM, \n",
    "                             Y_trainSVM,\n",
    "                             kernel=model.kernel_np)\n",
    "        print(f\"svm score is: {svm.score(X_trainSVM, Y_trainSVM)}\")\n",
    "        # -------------------------------------------------------------\n",
    "        alpha_list = []\n",
    "        for item in new_alphas:\n",
    "            alpha_list.append([item])\n",
    "        print(f\"new alphas are: {torch.tensor(alpha_list)}\")\n",
    "        model.update_alpha(torch.tensor(alpha_list))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "several hyperparameters are adjustable, refer to them in block 7.\n",
    "\n",
    "HID_DIM = 16   # this refers to the hidden size of Rnn in both encoder/ decoder \n",
    "N_LAYERS = 2   # this refers to the numbers of layers of Rnn in both encoder/ decoder\n",
    "TARG_LENGTH = 16 # this refers to the output sequence length of seq2seq2 module.\n",
    "\n",
    "\n",
    "block 8 randomly generates data required for training,\n",
    "this can be used to debug the pipeline,\n",
    "in actual training process, there is no need to run block 8.\n",
    "\n",
    "\n",
    "notice that in block 6 -> kernel_np(self, x1, x2), \n",
    "and in block 10: before  model.update_alpha(torch.tensor(alpha_list))\n",
    "crucial datatype transformation should be undertaken\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
